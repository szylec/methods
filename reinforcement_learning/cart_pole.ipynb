{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFrom OpenAI gym.\\nhttps://gym.openai.com/envs/CartPole-v1/\\n\\nA pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. \\nThe system is controlled by applying a force of +1 or -1 to the cart. \\nThe pendulum starts upright, and the goal is to prevent it from falling over.\\nA reward of +1 is provided for every timestep that the pole remains upright. \\nThe episode ends when the pole is more than 15 degrees from vertical, \\nor the cart moves more than 2.4 units from the center.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset\n",
    "'''\n",
    "From OpenAI gym.\n",
    "https://gym.openai.com/envs/CartPole-v1/\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. \n",
    "The system is controlled by applying a force of +1 or -1 to the cart. \n",
    "The pendulum starts upright, and the goal is to prevent it from falling over.\n",
    "A reward of +1 is provided for every timestep that the pole remains upright. \n",
    "The episode ends when the pole is more than 15 degrees from vertical, \n",
    "or the cart moves more than 2.4 units from the center.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from tensorflow import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "EPISODES = 10\n",
    "\n",
    "state_initial = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03348143 -0.17029101  0.01537828  0.34083598]\n",
      "1.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "action = np.random.random_integers(low=0, high=1)\n",
    "env.reset()\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "print(next_state)\n",
    "print(reward)\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]  # angle of the pole and position of the cart\n",
    "action_size = env.action_space.n  # 0 or 1 to the cart, pushing it left or right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "LEARNING_RATE = 0.001\n",
    "# ------------------------------------\n",
    "\n",
    "def build_model(state_size, action_size):\n",
    "    # Neural Net for Deep-Q learning.\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(state_size,)))\n",
    "    model.add(tf.keras.layers.Dense(24, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(24, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(action_size, activation='linear'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        metrics=['mae'],\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=LEARNING_RATE))\n",
    "    return model\n",
    "\n",
    "model = build_model(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f3069c547ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solves the cartpole-v1 enviroment on OpenAI gym using policy search\n",
    "Same algorithm as for cartpole-v0\n",
    "A neural network is used to store the policy\n",
    "At the end of each episode the target value for each taken action is\n",
    "updated with the total normalized reward (up to a learning rate)\n",
    "Then a standard supervised learning backprop on the entire batch is\n",
    "executed\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import numpy.matlib \n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#initialize neural network to store policy\n",
    "ActorNet = Sequential()\n",
    "ActorNet.add(Dense(200,init='he_normal',input_dim=4,activation='relu'))\n",
    "ActorNet.add(Dense(200,init='he_normal',activation='relu'))\n",
    "ActorNet.add(Dense(2,init='he_normal',activation='sigmoid'))\n",
    "ActorNet.compile(loss='mse',optimizer='RMSprop',metrics=['mae'])\n",
    "\n",
    "NumEpisodes = 300\n",
    "\n",
    "#load environment\n",
    "env = gym.make('CartPole-v1')\n",
    "env = gym.wrappers.Monitor(env, 'monitor')\n",
    "\n",
    "TotalReward = 0\n",
    "BufferSize = 0\n",
    "eps = 1\n",
    "\n",
    "#start learning\n",
    "for episode in range(NumEpisodes):\n",
    "\n",
    "    #initial state\n",
    "    observation = env.reset() #observe initial state\n",
    "\n",
    "    States = []\n",
    "    ActionValues = []\n",
    "    Actions = []\n",
    "\n",
    "    t = 0\n",
    "    loss = 0\n",
    "    EpisodeReward = 0\n",
    "\n",
    "    #decrease epsilon after each episode\n",
    "    eps -= 0.01\n",
    "    if eps<0:\n",
    "        eps = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        #show graphical environment\n",
    "        #env.render()\n",
    "\n",
    "        #evaluate NN to find action probabilities for current state\n",
    "\n",
    "        #normalize inputs\n",
    "        observation[0] /= 2.5\n",
    "        observation[1] /= 2.5\n",
    "        observation[2] /= 0.2\n",
    "        observation[3] /= 2.5\n",
    "\n",
    "        ActionValue = ActorNet.predict(observation.reshape(1,4),verbose=0).reshape(2,)\n",
    "\n",
    "        #select best action eps-greedy with decay\n",
    "        greedy = np.random.random()\n",
    "        if greedy < eps:\n",
    "            Action = np.random.randint(2)\n",
    "        else:\n",
    "            Action = np.argmax(ActionValue)\n",
    "        \n",
    "        #execute action\n",
    "        observation_new, reward, done, info = env.step(Action)\n",
    "\n",
    "        #normalize reward, maximum reward per episode is 500\n",
    "        reward /= 500.0\n",
    "\n",
    "        EpisodeReward += reward\n",
    "        \n",
    "        #save current movement in memory to assign rewards at end of episode\n",
    "        States.append(observation)\n",
    "        ActionValues.append(ActionValue)\n",
    "        Actions.append(Action)\n",
    "\n",
    "        #update state\n",
    "        observation = observation_new\n",
    "\n",
    "        #next time step\n",
    "        t += 1\n",
    "\n",
    "        #end episode\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    #update finished episode memory with new reward\n",
    "    #only update action value for actions that were taken, leave others unchanged\n",
    "    alpha = 0.1\n",
    "    for i in range(t):\n",
    "        ActionValues[i][Actions[i]] = ActionValues[i][Actions[i]] * (1-alpha) + EpisodeReward * alpha\n",
    "\n",
    "    #update weights of NN based on last completed episode\n",
    "    batch_in = np.empty([t,4]) #input state\n",
    "    batch_tar = np.empty([t,2]) #target action values\n",
    "    for i in range(t):\n",
    "        batch_in[i] = States[i]\n",
    "        batch_tar[i] = ActionValues[i]\n",
    "    loss += ActorNet.train_on_batch(batch_in, batch_tar)[0]\n",
    "\n",
    "    print('Episode {0}, reward = {1}'.format(episode,EpisodeReward))\n",
    "\n",
    "    TotalReward += EpisodeReward\n",
    "\n",
    "print('Total reward = {0}'.format(TotalReward))\n",
    "#ActorNet.save('CPv1_model.h5')\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
